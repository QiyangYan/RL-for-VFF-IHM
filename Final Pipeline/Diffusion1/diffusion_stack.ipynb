{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# !python --version\n",
    "# !pip3 install torch==1.13.1 torchvision==0.14.1 diffusers==0.18.2 \\\n",
    "# scikit-image==0.19.3 scikit-video==1.1.11 zarr==2.12.0 numcodecs==0.10.2 \\\n",
    "# pygame==2.1.2 pymunk==6.2.1 gym==0.26.2 shapely==1.8.4 \\\n",
    "# &> /dev/null # mute output"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# !pip install opencv-python-headless scikit-video"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "#@markdown ### **Imports**\n",
    "# diffusion policy import\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import zarr\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# env import\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import pymunk\n",
    "import pymunk.pygame_util\n",
    "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
    "from pymunk.vec2d import Vec2d\n",
    "import shapely.geometry as sg\n",
    "import cv2\n",
    "import skimage.transform as st\n",
    "from skvideo.io import vwrite\n",
    "from IPython.display import Video\n",
    "import gdown\n",
    "import os"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# import pickle\n",
    "# with open(f'/Users/qiyangyan/Desktop/Diffusion/Demonstration/VFF-random_9884demos', 'rb') as f:\n",
    "#   dataset_path = pickle.load(f)\n",
    "\n",
    "import pickle\n",
    "with open(f'/Users/qiyangyan/Desktop/Diffusion/Demonstration/VFF-bigSteps', 'rb') as f:\n",
    "  dataset_path = pickle.load(f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "#@markdown ### **Dataset Demo**\n",
    "from my_module import PushTStateDataset\n",
    "\n",
    "# parameters\n",
    "# pred_horizon = 16\n",
    "# obs_horizon = 2\n",
    "# action_horizon = 8\n",
    "\n",
    "pred_horizon = 4\n",
    "obs_horizon = 2\n",
    "action_horizon = 2\n",
    "\n",
    "#|o|o|                             observations: 2\n",
    "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
    "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
    "\n",
    "# create dataset from file\n",
    "dataset = PushTStateDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon\n",
    ")\n",
    "# save training data statistics (min, max) for each dim\n",
    "stats = dataset.stats\n",
    "print(dataset.__len__())\n",
    "\n",
    "\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['obs'].shape:\", batch['obs'].shape)\n",
    "print(\"batch['action'].shape\", batch['action'].shape)\n",
    "\n",
    "print(np.shape(batch['obs'].flatten(start_dim=1)))\n",
    "print(batch['obs'].flatten(start_dim=1)[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "#@markdown ### **Network**\n",
    "#@markdown\n",
    "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
    "#@markdown as the noies prediction network\n",
    "#@markdown\n",
    "#@markdown Components\n",
    "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
    "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
    "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
    "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
    "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
    "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
    "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        print(np.shape(x), h)\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "#@markdown ### **Network Demo**\n",
    "\n",
    "# observation and action dimensions corrsponding to\n",
    "# the output of PushTEnv\n",
    "obs_dim = 24\n",
    "action_dim = 2\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# example inputs\n",
    "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
    "diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "print(np.shape(noised_action))\n",
    "print(np.shape(pred_horizon))\n",
    "\n",
    "# the noise prediction network\n",
    "# takes noisy action, diffusion iteration and observation as input\n",
    "# predicts the noise added to action\n",
    "noise = noise_pred_net(\n",
    "    sample=noised_action,\n",
    "    timestep=diffusion_iter,\n",
    "    global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "# illustration of removing noise\n",
    "# the actual noise removal is performed by NoiseScheduler\n",
    "# and is dependent on the diffusion noise schedule\n",
    "denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cpu')\n",
    "_ = noise_pred_net.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"VariableFriction-v8\")\n",
    "env_dict = env.reset()[0]\n",
    "print(env_dict)\n",
    "for key in env_dict.keys():\n",
    "  print(np.shape(env_dict[key]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "import os\n",
    "import gdown\n",
    "import torch\n",
    "\n",
    "# Set the path to the checkpoint file\n",
    "ckpt_path = '/Users/qiyangyan/Downloads/ema_noise_pred_net_epoch_99.ckpt'\n",
    "\n",
    "# Ensure the file exists\n",
    "if not os.path.isfile(ckpt_path):\n",
    "    print(f\"Checkpoint file {ckpt_path} does not exist.\")\n",
    "else:\n",
    "    # Load the state dictionary\n",
    "    state_dict = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    # Load the state dictionary into the model\n",
    "    ema_noise_pred_net = noise_pred_net\n",
    "    ema_noise_pred_net.load_state_dict(state_dict['model_state_dict'])\n",
    "\n",
    "    print('Pretrained weights loaded.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "#@markdown ### **Training**\n",
    "#@markdown\n",
    "#@markdown Takes about an hour. If you don't want to wait, skip to the next cell\n",
    "#@markdown to load pre-trained weights\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nobs = nbatch['obs'].to(device)\n",
    "                naction = nbatch['action'].to(device)\n",
    "                B = nobs.shape[0]\n",
    "\n",
    "                print(\"Check: \", nobs)\n",
    "\n",
    "                # observation as FiLM conditioning\n",
    "                # (B, obs_horizon, obs_dim)\n",
    "                obs_cond = nobs[:,:obs_horizon,:]\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "                obs_cond = obs_cond.flatten(start_dim=1)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(noise_pred_net.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_noise_pred_net = noise_pred_net\n",
    "ema.copy_to(ema_noise_pred_net.parameters())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "import rotations\n",
    "\n",
    "def discretize_action_to_control_mode_E2E(action):\n",
    "    \"\"\"\n",
    "    -1 ~ 1 maps to 0 ~ 1\n",
    "    \"\"\"\n",
    "    # Your action discretization logic here\n",
    "    # print(\"Action: \", action)\n",
    "    action_norm = (action + 1) / 2\n",
    "    # print(action_norm, action)\n",
    "    if 1 / 6 > action_norm >= 0:\n",
    "        print(\"| Slide up on right finger\")\n",
    "        control_mode = 0\n",
    "        friction_state = 1  # left finger high friction\n",
    "        pos_idx = 0\n",
    "    elif 2 / 6 > action_norm >= 1 / 6:\n",
    "        print(\"| Slide down on right finger\")\n",
    "        control_mode = 1\n",
    "        friction_state = 1\n",
    "        pos_idx = 1\n",
    "    elif 3 / 6 > action_norm >= 2 / 6:\n",
    "        print(\"| Slide up on left finger\")\n",
    "        control_mode = 2\n",
    "        friction_state = -1\n",
    "        pos_idx = 1\n",
    "    elif 4 / 6 > action_norm >= 3 / 6:\n",
    "        print(\"| Slide down on left finger\")\n",
    "        control_mode = 3\n",
    "        friction_state = -1\n",
    "        pos_idx = 0\n",
    "    elif 5 / 6 > action_norm >= 4 / 6:\n",
    "        print(\"| Rotate clockwise\")\n",
    "        control_mode = 4\n",
    "        friction_state = 0\n",
    "        pos_idx = 0\n",
    "        # print(\"Rotate\")\n",
    "    else:\n",
    "        assert 1 >= action_norm >= 5 / 6\n",
    "        print(\"| Rotate anticlockwise\")\n",
    "        control_mode = 5\n",
    "        friction_state = 0\n",
    "        pos_idx = 1\n",
    "        # print(\"Rotate\")\n",
    "    return friction_state, control_mode, pos_idx\n",
    "\n",
    "def compute_orientation_diff(goal_a, goal_b):\n",
    "    ''' get pos difference and rotation difference\n",
    "    left motor pos: 0.037012 -0.1845 0.002\n",
    "    right motor pos: -0.037488 -0.1845 0.002\n",
    "    '''\n",
    "    assert goal_a.shape == goal_b.shape, f\"Check: {goal_a.shape}, {goal_b.shape}\"\n",
    "    assert goal_a.shape[-1] == 7\n",
    "    goal_a[2] = goal_b[2]\n",
    "\n",
    "    d_pos = np.zeros_like(goal_a[..., 0])\n",
    "\n",
    "    delta_pos = goal_a[..., :3] - goal_b[..., :3]\n",
    "    d_pos = np.linalg.norm(delta_pos, axis=-1)\n",
    "\n",
    "    quat_a, quat_b = goal_a[..., 3:], goal_b[..., 3:]\n",
    "\n",
    "    euler_a = rotations.quat2euler(quat_a)\n",
    "    euler_b = rotations.quat2euler(quat_b)\n",
    "    if euler_a.ndim == 1:\n",
    "        euler_a = euler_a[np.newaxis, :]  # Reshape 1D to 2D (1, 3)\n",
    "    if euler_b.ndim == 1:\n",
    "        euler_b = euler_b[np.newaxis, :]  # Reshape 1D to 2D (1, 3)\n",
    "    euler_a[:,:2] = euler_b[:,:2]  # make the second and third term of euler angle the same\n",
    "    quat_a = rotations.euler2quat(euler_a)\n",
    "    quat_a = quat_a.reshape(quat_b.shape)\n",
    "\n",
    "    # print(quat_a, quat_b)\n",
    "    quat_diff = rotations.quat_mul(quat_a, rotations.quat_conjugate(quat_b))  # q_diff = q1 * q2*\n",
    "    angle_diff = 2 * np.arccos(np.clip(quat_diff[..., 0], -1.0, 1.0))\n",
    "    d_rot = angle_diff\n",
    "    return d_pos, d_rot\n",
    "\n",
    "def change_friction_full_obs(action, env):\n",
    "    '''\n",
    "    one while loop might have an early-finish when the position of finger is reached\n",
    "    the friction change might not be completed\n",
    "    '''\n",
    "    start_t = time.time()\n",
    "    while True:\n",
    "        next_env_dict, distance, terminated, truncated, info = env.step(action)\n",
    "        # print(\"two finger position: \", next_env_dict['observation'][0], next_env_dict['observation'][2])\n",
    "        if terminated is True:\n",
    "            return next_env_dict, distance, terminated, truncated, info\n",
    "            # return False, distance, next_env_dict[\"observation\"][pos_idx*2]  # false meaning not complete\n",
    "        if distance[\"action_complete\"]:\n",
    "            break\n",
    "        if time.time() - start_t > 2.5:\n",
    "            terminated = True\n",
    "            return next_env_dict, distance, terminated, truncated, info\n",
    "    return next_env_dict, distance, terminated, truncated, info\n",
    "\n",
    "def friction_change(friction_state, env):\n",
    "    friction_action_1 = [2, 0, True]\n",
    "    friction_action_2 = [2, friction_state, True]\n",
    "    # input(\"Press Enter to continue...\")\n",
    "    new_obs, rewards, terminated, _, infos = change_friction_full_obs(np.array(friction_action_1),\n",
    "                                                                                    env)\n",
    "    if terminated:\n",
    "        print(\"terminate at friction change to high\")\n",
    "        return new_obs, rewards[\"RL_IHM\"], terminated, _, infos\n",
    "    # input(\"press\")\n",
    "    new_obs, rewards, terminated, _, infos = change_friction_full_obs(np.array(friction_action_2),\n",
    "                                                                                    env)\n",
    "    if terminated:\n",
    "        print(\"terminate at friction change to low\")\n",
    "    return new_obs, rewards[\"RL_IHM\"], terminated, _, infos\n",
    "\n",
    "def pick_up(inAir, env):\n",
    "    t1 = time.time()\n",
    "    # pick_up_action = [0, -2, False]\n",
    "    pick_up_action = [0, 2, False]\n",
    "    # print(\"start picking\")\n",
    "    'The position position-controlled finger reaches the middle'\n",
    "    while True:  # for _ in range(105):\n",
    "        # pick_up_action[0] += 0.01\n",
    "        pick_up_action[0] = 0.9557\n",
    "        # print(pick_up_action[0])\n",
    "        state, reward, _, _, _ = env.step(np.array(pick_up_action))\n",
    "        # while not reward['action_complete']:\n",
    "        #     state, reward, _, _, _ = self.env.step(np.array(pick_up_action))\n",
    "        # print(state[\"observation\"][0])\n",
    "        if abs(state[\"observation\"][0] - pick_up_action[0]) < 0.003:\n",
    "            print(\"Pick up complete\")\n",
    "            break\n",
    "    # print(\"closing\")\n",
    "    'Wait until the torque-controlled finger reaches the middle'\n",
    "    for _ in range(50):\n",
    "        state, reward, _, _, _ = env.step(np.array(pick_up_action))\n",
    "    # print(\"pick up complete --------\")\n",
    "\n",
    "    'Wait until the finger raised to air'\n",
    "    lift_action = [0, -3, False]\n",
    "    if inAir is True:\n",
    "        print(\"Lifting the block\")\n",
    "        while True: # for _ in range(120):\n",
    "            state, reward, _, _, _ = env.step(np.array(lift_action))\n",
    "            if reward[\"action_complete\"]:\n",
    "                break\n",
    "    return state, reward\n",
    "\n",
    "def ihm_step(env, action, last_friction_state):\n",
    "  friction_state, control_mode_dis, pos_idx = discretize_action_to_control_mode_E2E(action[1])\n",
    "  if last_friction_state == friction_state:\n",
    "    obs, r_dict, terminated, _, info_ = env.step(action)\n",
    "  else:\n",
    "    obs, r_dict, terminated, _, info_ = friction_change(friction_state, env)\n",
    "    if not terminated:\n",
    "      obs, r_dict, terminated, _, info_ = env.step(action)\n",
    "\n",
    "  _, angle_diff = compute_orientation_diff(np.array(obs['desired_goal'][:7]), np.array(obs['achieved_goal'][:7]))\n",
    "  radi_diff = obs['desired_goal'][7:9] - obs['achieved_goal'][7:9]\n",
    "  obs['observation'][-1] = angle_diff\n",
    "  obs['observation'][-2] = radi_diff[1]\n",
    "  obs['observation'][-3] = radi_diff[0]\n",
    "  return obs, r_dict, terminated, terminated, info_, friction_state"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@markdown ### **Inference**\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# limit enviornment interaction to 200 steps before termination\n",
    "max_steps = 800\n",
    "\n",
    "env = gym.make(\"VariableFriction-v8\", render_mode=\"human\")\n",
    "\n",
    "# get first observation\n",
    "# obs_dict, info = env.reset(seed=100000)\n",
    "obs_dict, info = env.reset()\n",
    "obs_dict, _ = pick_up(False, env)\n",
    "obs = obs_dict['observation']\n",
    "\n",
    "# keep a queue of last 2 steps of observations\n",
    "obs_deque = collections.deque(\n",
    "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "# save visualization and rewards\n",
    "rewards = list()\n",
    "done = False\n",
    "step_idx = 0\n",
    "last_friction_state = 0\n",
    "\n",
    "\n",
    "\n",
    "with tqdm(total=max_steps, desc=\"Eval PushTStateEnv\") as pbar:\n",
    "    while not done:\n",
    "        B = 1\n",
    "        # stack the last obs_horizon (2) number of observations\n",
    "        obs_seq = np.stack(obs_deque)\n",
    "        # normalize observation\n",
    "        nobs = normalize_data(obs_seq, stats=stats['obs'])\n",
    "        # device transfer\n",
    "        nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
    "\n",
    "        # infer action\n",
    "        with torch.no_grad():\n",
    "            # reshape observation to (B,obs_horizon*obs_dim)\n",
    "            obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            # initialize action from Guassian noise\n",
    "            noisy_action = torch.randn(\n",
    "                (B, pred_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            # init scheduler\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                # predict noise\n",
    "                noise_pred = ema_noise_pred_net(\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "\n",
    "                # inverse diffusion step (remove noise)\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        # unnormalize action\n",
    "        naction = naction.detach().to('cpu').numpy()\n",
    "        # (B, pred_horizon, action_dim)\n",
    "        naction = naction[0]\n",
    "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "        # only take action_horizon number of actions\n",
    "        start = obs_horizon - 1\n",
    "        end = start + action_horizon\n",
    "        action = action_pred[start:end,:]\n",
    "        # (action_horizon, action_dim)\n",
    "\n",
    "        # execute action_horizon number of steps\n",
    "        # without replanning\n",
    "\n",
    "        print((action[:, 0]+1)/2)\n",
    "        for i in range(len(action)):\n",
    "            # stepping env\n",
    "            obs_dict, reward, done, _, info, friction_state = ihm_step(env, action[i], last_friction_state)\n",
    "            obs = obs_dict['observation']\n",
    "\n",
    "            # print(obs)\n",
    "\n",
    "            last_friction_state = friction_state\n",
    "            # save observations\n",
    "            obs_deque.append(obs)\n",
    "            # and reward/vis\n",
    "            rewards.append(0)\n",
    "\n",
    "            # update progress bar\n",
    "            step_idx += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(reward=reward)\n",
    "            if step_idx > max_steps:\n",
    "                done = True\n",
    "            if done or len(obs) >= max_steps:\n",
    "                break\n",
    "        \n",
    "        # input(\"Press to continue\")\n",
    "\n",
    "env.close()\n",
    "env.reset()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"VariableFriction-v8\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment\n",
    "obs_dict, info = env.reset(seed=100000)\n",
    "print(obs_dict, info)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ef7ad3193d81baf5621dfa8fef3ebbfa6fcd64e5d62270724ced259e6abb473"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
